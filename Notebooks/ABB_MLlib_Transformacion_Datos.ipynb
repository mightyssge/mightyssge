{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mightyssge/mightyssge/blob/main/Notebooks/ABB_MLlib_Transformacion_Datos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <img style=\"float: left; padding: 0px 10px 0px 0px;\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/a/a0/Universidad_de_Lima_logo.png/220px-Universidad_de_Lima_logo.png\"  width=\"120\" />  MLlib: Transformaciones de Datos\n",
        "**Profesor:** Enver G. Tarazona Vargas <br>\n",
        "**Curso:** Analítica con Big Data <br>\n",
        "**FACULTAD DE INGENIERÍA - CARRERA DE INGENIERÍA DE SISTEMAS**<br>"
      ],
      "metadata": {
        "id": "vTdGK9Vk8Hzg"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9eQZTS11hij"
      },
      "source": [
        "Usualmente no se tiene los datos en un formato conveniente. Una gran parte del trabajo con datos consiste en usar el conocimiento de un dominio determinado para saber cómo manejar los datos (eliminar algunos datos faltantes, realizar \"feature engineering\", transformar datos, etc.)\n",
        "\n",
        "Spark tiene métodos para realizar estas transformaciones: http://spark.apache.org/docs/latest/ml-features.html"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pyspark"
      ],
      "metadata": {
        "id": "DBrGGV8hSPl0"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c"
      ],
      "metadata": {
        "id": "wz-cqCEsWAsT",
        "outputId": "4809476b-a768-438e-ac32-e1d6dc8936e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'c' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3235490055.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'c' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMRiZnWJ1hik"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPcpJ3RD1hiw"
      },
      "source": [
        "## 1.&nbsp;Atributos categóricos a numéricos usando índices\n",
        "\n",
        "Se puede utilizar `StringIndexer` para convertir atributos categóricos (no numéricos) en atributos numéricos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6uu_9rh1hix"
      },
      "source": [
        "from pyspark.ml.feature import StringIndexer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Ejemplo*: creación manual de un dataframe donde el atributo \"sucursal\" es categórico (no es numérico)"
      ],
      "metadata": {
        "id": "ZbARZfJWSdNN"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-bzy7aZ2oa0"
      },
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "df = spark.createDataFrame([Row(ID=0, sucursal=\"A\", venta=10000), Row(ID=1, sucursal=\"B\", venta=9000),\n",
        "                            Row(ID=2, sucursal=\"C\", venta=15000), Row(ID=3, sucursal=\"A\", venta=14000),\n",
        "                            Row(ID=4, sucursal=\"A\", venta=12000), Row(ID=5, sucursal=\"C\", venta=19000),\n",
        "                            Row(ID=6, sucursal=\"D\", venta=11500), Row(ID=7, sucursal=\"D\", venta=5000)\n",
        "])\n",
        "df.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se convertirá la columna \"sucursal\" en numérica utilizando `StringIndexer`. Primero se indica cuál es la columna de entrada (`inputCol`) y cuál será la columna de salida (`indiceSucursal`)"
      ],
      "metadata": {
        "id": "zB9ApQ9dT2Gf"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utk-xKIw2znZ"
      },
      "source": [
        "# La columna con categoría indexada (numérica) se llamará \"indiceCategoria\"\n",
        "indexador = StringIndexer(inputCol=\"sucursal\", outputCol=\"indiceSucursal\")\n",
        "\n",
        "# Obtener las asociaciones entre categorías y valores numéricos (mapa)\n",
        "modeloIndexador = indexador.fit(df)\n",
        "# Mostrar las etiquetas que se mapean como (0, 1, 2)\n",
        "modeloIndexador.labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modeloIndexador"
      ],
      "metadata": {
        "id": "uIXAemFOWb31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Luego se transforma los datos según los índices generados. Notar que se utiliza `transform` para realizar esta transformación"
      ],
      "metadata": {
        "id": "upQ0sdijUkLK"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XiOKTx87boy"
      },
      "source": [
        "# Transformar los datos según los índices generados\n",
        "df2 = modeloIndexador.transform(df)\n",
        "df2.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Luego de esta transformación, se puede utilizar el índice `indiceSucursal` como entrada numérica para algún algoritmo de Machine Learning."
      ],
      "metadata": {
        "id": "Qg1dNpyBdIsH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.&nbsp;Atributos categóricos a numéricos usando One-hot Encoding"
      ],
      "metadata": {
        "id": "SFDL-3K4VLzw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La idea de one-hot encoding es mapear cada categoría a un vector binario con un solo valor que indique la presencia de un atributo particular (una característica específica)."
      ],
      "metadata": {
        "id": "SpYm1K7udTo0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder"
      ],
      "metadata": {
        "id": "YKo8mZPBVl_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data frame original\n",
        "df.show()"
      ],
      "metadata": {
        "id": "53Zg6yTZVLdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1. Forma 1: Manipulación directa\n",
        "\n",
        "En este caso, se puede manipular directamente el indexador y el conversor a one-hot encoding"
      ],
      "metadata": {
        "id": "o5wl7Yi6aA4c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear y aplicar un indexador\n",
        "indexador = StringIndexer(inputCol=\"sucursal\", outputCol=\"indiceSucursal\")\n",
        "# Transformar y aplicar el indexador al DataFrame\n",
        "df2 = indexador.fit(df).transform(df)\n",
        "\n",
        "# Definir One-Hot encoder\n",
        "one_hot_encoder = OneHotEncoder(inputCol=\"indiceSucursal\", outputCol=\"onehotSucursal\")\n",
        "# Transformar y aplicar OneHotEncoder al DataFrame\n",
        "df3 = one_hot_encoder.fit(df2).transform(df2)\n",
        "df3.show()"
      ],
      "metadata": {
        "id": "Z7XFN5ppX_NC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explicación de la Columna `onehotSucursal`\n",
        "\n",
        "En la salida de la columna `onehotSucursal`, observamos la representación de los datos en formato **Sparse Vector** (vector disperso). Spark utiliza este formato para optimizar el almacenamiento de vectores con muchos ceros, como es el caso en las codificaciones One-Hot.\n",
        "\n",
        "### Estructura de la Columna `onehotSucursal`\n",
        "\n",
        "La salida tiene el siguiente formato: `(longitud del vector, [posiciones], [valores])`\n",
        "\n",
        "1. **Longitud del vector:** Representa la cantidad total de elementos en el vector. En nuestro caso, el vector tiene 3 elementos.\n",
        "2. **Posiciones:** Indica las posiciones en las que existen valores distintos de cero.\n",
        "3. **Valores:** Lista los valores correspondientes a las posiciones indicadas, generalmente `1.0` en el caso de One-Hot Encoding.\n",
        "\n",
        "### Ejemplos de Interpretación\n",
        "\n",
        "- **(3, [0], [1.0])**:\n",
        "  - Es un vector de longitud 3: `[1.0, 0.0, 0.0]`.\n",
        "  - Esto significa que la sucursal tiene un índice de `0` (asignado a la sucursal \"A\").\n",
        "\n",
        "- **(3, [2], [1.0])**:\n",
        "  - Es un vector de longitud 3: `[0.0, 0.0, 1.0]`.\n",
        "  - Esto significa que la sucursal tiene un índice de `2` (asignado a la sucursal \"D\").\n",
        "\n",
        "- **(3, [], [])**:\n",
        "  - Este caso indica un vector en el cual no se asignó ningún valor distinto de cero, común en ciertas salidas específicas de Spark.\n",
        "\n",
        "### Importancia del Formato Sparse Vector\n",
        "\n",
        "Este formato permite almacenar únicamente los valores necesarios, optimizando el espacio y mejorando el rendimiento en el procesamiento de datos con Spark. En otros entornos de programación, como pandas o scikit-learn, se suele utilizar la matriz completa, mostrando explícitamente todos los ceros y unos.\n"
      ],
      "metadata": {
        "id": "rD3F5ltOCer7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2. Forma 2: Usando un Pipeline\n"
      ],
      "metadata": {
        "id": "5z69-Udraf-v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml import Pipeline"
      ],
      "metadata": {
        "id": "tm5rojOmZjer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Indexador sin aplicarlo al DataFrame\n",
        "string_indexer = StringIndexer(inputCol=\"sucursal\", outputCol=\"indiceSucursal\")\n",
        "# OneHotEncoder sin aplicarlo al Dataframe\n",
        "one_hot_encoder = OneHotEncoder(inputCol=\"indiceSucursal\", outputCol=\"onehotSucursal\")\n",
        "\n",
        "# Pipeline con las etapas\n",
        "pipeline = Pipeline(stages=[string_indexer,\n",
        "                            one_hot_encoder])\n",
        "\n",
        "# Obtener las asociaciones usando el pipeline\n",
        "df2 = pipeline.fit(df)\n",
        "\n",
        "# Transformar el DataFrame\n",
        "df3 = df2.transform(df)\n",
        "\n",
        "# Resultado\n",
        "df3.show()"
      ],
      "metadata": {
        "id": "n7tOGd7QZejP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3M6idxnR1hi2"
      },
      "source": [
        "## 3.&nbsp;Generación de un vector columna (combinando otras columnas)\n",
        "\n",
        "`VectorAssembler` combina un conjunto de columnas en un solo vector columna. Es útil para combinar atributos originales con aquellos generados por diferentes transformaciones aplicadas en PySpark. Esto es necesario para tener el formato que los modelos de ML de Spark utilizan.\n",
        "\n",
        "`VectorAssembler` acepta los siguientes tipos de columnas: todos los tipos numéricos, tipos Booleanos, tipos vector. En cada fila, los valores de las columnas de entrada serán concatenados en un vector de un orden especificado.\n",
        "\n",
        "Ejemplo: Si se tiene un DataFrame con las columnas id, campo1, campo2, campos3, y valor:\n",
        "\n",
        "     id | campo1 | campo2 |   campos3   | valor\n",
        "    ----|--------|--------|-------------|------\n",
        "    204 |   18   |   1.0  | [3, 10, 20] |  5.9\n",
        "\n",
        "donde `campos3` es una columna de vectores que contiene tres atributos. Se desea combinar `campo1`, `campo2` y `campos3` en un solo vector de atributos llamado `vatributos` para ser usado como predictor de `valor`. Si se indica que las columnas de entrada de `VectorAssembler` son `campo1`, `campo2` y `campos3`, y que la columna de salida es `valor`, luego de la transformación se obtendrá lo siguiente:\n",
        "\n",
        "     id | campo1 | campo2 |   campos3   | valor |     vatributos\n",
        "    ----|--------|--------|-------------|-------|----------------------\n",
        "    204 |   18   |   1.0  | [3, 10, 20] |  5.9  | [18, 1.0, 3, 10, 20]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Mo1k8uX-q7A"
      },
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.linalg import Vectors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAxnr-JX-scC"
      },
      "source": [
        "# Vector denso en Spark\n",
        "vector = Vectors.dense([3, 10, 20])\n",
        "vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYZqzQgP1hi2"
      },
      "source": [
        "# Creación de un data frame (de una fila)\n",
        "df = spark.createDataFrame([(204, 18, 1.0, vector, 5.9),\n",
        "                            (205, 25, 3.5, vector, 6.7)],\n",
        "                           [\"id\", \"campo1\", \"campo2\", \"campos3\", \"valor\"])\n",
        "df.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyrQyHyc1hi5"
      },
      "source": [
        "# Objeto que juntará columnas para crear una sola columna\n",
        "assembler = VectorAssembler(inputCols=[\"campo1\", \"campo2\", \"campos3\"],\n",
        "                            outputCol=\"vatributos\")\n",
        "\n",
        "# Transformar los datos según la columna creada\n",
        "df2 = assembler.transform(df)\n",
        "df2.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Az8pxJGTAtkn"
      },
      "source": [
        "df2.show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlNFZIxL1hi7"
      },
      "source": [
        "# Seleccionar solo las columnas vatributos y valor (usual como entrada a algoritmos supervizados)\n",
        "df2.select(\"vatributos\", \"valor\").show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}